# openai-api

## batch
+ [[diary:2025-02-19]]
- *structured output* 의 `schema` 잘못 정의 등으로 실패하는 경우 batch 는 토큰을 소모하지 않음(과금 no)
- batch 내부에서의 각 요청의 성공/실패 여부와 상관없이 모든 요청에 대해서 처리가 되면 `completed` 상태
- batch 는 원하는 작업을 걸어놓을때부터 [[#token]] 카운트가 시작됨, 때문에 한도 내에서 요청을 하지 않으면 batch 요청이 실패함
- **생각**
  - 하나의 batch 에 토큰 카운트를 꽉꽉채우는건 여러 작업(하나의 작업이 쪼개진게 아닌 작업 자체가)을 동시에 분할로 진행할 때 유연성이 떨어진다
  - 큰 배치파일이 실행 중인 경우 진행된(진행/전체) 요청수와 별개로 전체 token 수만큼 lock 되는 것으로 보임
  - 각각 다양한 토큰 사이즈로 쪼개서 동시에 진행을 가능하게 하는 것도 하나의 전략
  - prompt cache 관점에서는 조금 손해가 있을 수 있다

## token :token:
- token 계산은 tiktoken 라이브러리를 통해 계산가능
- model 별로 다르게 계산됨
- 

## link
- [[openai]]
