### Data Centric

## 학습
- 학습 순서
  - 데이터 기획
  - 데이터 수집
  - 데이터 라벨링
  - 데이터 클렌징
  - 데이터 마무리
  - 데이터 제작 실습

### Data-Centric AI란 무엇인가?
- AI System = Code + Data
- 데이터 품질 -> 성능 향상
  - 데이터 어노테이션 일관성이 꺠지면 노이즈로 인식됨
- AI 프로젝트는 66~80%가 데이터 관련 작업
  - 그외에 알고리즘, 모델 학습, 모델 조정, 서비스 배포등
- Model-Centric AI
  - 데이터가 많아지면 노이즈는 묻힌다는 개념
- Data-Centric
  - 데이터 품질에 집중
- AI 서비스 개발 과정
  - 프로젝트 셋업
  - 데이터 준비
  - 모델 훈련
  - 배포
- 서비스 출시 이전 중요도
  - 모델 1:1 데이터
- 서비스 출시 이후 중요도
  - 모델 2:8 데이터
- MLOps
  - 데이터 구축을 위한 인프라
  - google, nvidia 참조
- 요즘은 파인튜닝
  - 요즘 LoRA
  - 적은 데이터로도 최적화가 가능
- 연구분야
  - 윤리
  - 품질
    - 이상 탐지및 제거
    - 에러 탐지 및 수정
      + https://labelerrors.com
    - 데이터 증강
      - back translation
    - 피처 엔지니어링
    - 컨센서스 라벨링
      - 다수의 라벨러로 부터 얻은 라벨들의 일관성을 높이는 방법
    - 액티브 러닝
      - 모든 데이터가 아니라 중요한 데이터를 식별하고 학습하는 방식
    - 커리큘럼 학습
      - 쉬운 데이터부터 차근차근 학습할 수 있도록 학습 순서 조절하는 방식
  - 평가
    - 테스트데이터 추가로 평가
    - 부분집합으로 평가
    - 손상된 학습데이터 식별

### Data-Centric AI의 미래
- foundation model 시대
  - multilingual
    - 과거는 unilingual
    - transformer 이전(LSTM)에는 모델도 키우기 어려움
    - transforemr 이후로는 모델이 커지면서 multilingual이 대세
    - 성능도 unilingual 을 뛰어 넘음
  - multimodal
    - LMM Large Multimodal Model
    - LLM 이후로 멀티모달의 성능이 향상됨
    - VQA Visual Question Answering
      - 이미지 -> 텍스트 디스크립션 표현
    - Multimodal Data VidLN
      - 동영상 -> 텍스트 디스크립션 표현
    - VDialogUE
      - 이미지+텍스트
    - 로봇 센서등 여러 입력을 기반으로한 데이터들이 많음
  - synthetic data
    - 합성 데이터
    - 모델이 생성한 데이터 -> 다른 모델 훈련
      - chatgpt 4로 생성된 데이터가 이미 사람의 것을 넘어섬
    - 싸고 빠르다
    - 어려운 조건의 데이터도 생성가능
    - 데이터 편향 회피
    - 사례
      - 자율주행
      - VQA
      - 오디오도 생성

### 데이터 구축 프로세스 소개
- 데이터 구축 프로세스
  - 수집
    - 방법
      - 원시(raw) 데이터
      - 수집
      - 크롤링
      - 오픈소스
      - 크라우드 소싱
    - 데이터 타당성 검토
      - 다양성
      - 편향 방지
      - 윤리 준수
      - 수집 방법
      - 법, 제도 준수
  - 전처리
    - 원천(source) 데이터
    - 품질 확보
      - 품질 기준 마련
      - 개인정보 비식별화
      - 중복성 방지
        - 유사 데이터 및 특성이 없는 데이터 제거
    - 품질 검사 지표
      - 적합성, 신뢰성, 정합성 등등
    - 스키마 설계
      - 어떻게 저장할지 구조 설계
      - 의미 분할, 요약시 압축 정도, 추출할 것인지 추상화할 것인지
      - 감성 분석, 전체 텍스트에 하나만 혹은 요소별로 라벨링할 것인지
      - 예시
        - 문서 요약
          - 압축할 양
          - 추출 추출 단어 수
          - 추상화 생성, 타겟 글자 수
  - 라벨링
    - 라벨링을 할 기준을 마련
    - 가이드라인 작성 -> 작업자 교육 -> 라벨링 진행
      - 데이터 구축 목적 정의
      - 사용 용어 정의
      - 예시를 통한 체계 설명
      - 버전 관리
    - 단계
      - 파일럿 테스트를 통한 소규모 데이터셋 구축
        - 문제, 엣지케이스발견
      - 잘 설계된 데이터 라벨링 툴을 사용해서 라벨링하는 것이 핵심
        - 잘 설계된 툴
          - Quality Control
            - 일관성 있고 정확한 데이터 생성가능여부
          - Efficientcy
          - Scalability
  - 클렌징
    - 라벨링 된 데이터의 품질 검수
    - 기준이하 폐기
    - 종류
      - 내재적 요소 검수
        - 가이드 라인이 준수 되었는지
        - 작업자들간 라벨링이 일치하는지
        - IAA Inter Annotator Agreement 평가 지표를 통해 확인
      - 외재적 요소 검수
        - 다양성, 신뢰성, 충분성, 윤리 적합성을 살펴보는 과정
        - 외부 자문위원들도 있다
  - 스플릿
    - 학습, 검증, 테스트셋으로 분할
  - 릴리즈
    - 배포내용, 있으면 좋다
      - 라벨링 데이터
      - 버전 및 로그
      - 샘플데이터 형태
      - 데이터 분석서
      - 품질 평가서
- 데이터 구축 프로세스의 진행
  - 사이클이 존재
    - 폭포수(Waterfall) 모델
      - 순차적
      - 이터레이션이 별로 필요없는 경우
    - 나선형(Spiral) 모델
      - 빠르게 이터레이션
      - 장기간
  - 과정의 이동간에 유연성이있다

### 데이터 구축 기획서 작성
- 데이터 구축 기획서의 작성
  - 기획
    - 어던 방식으로 수집되는가
    - 무슨 문제를 해결하는가
    - 수집 필요 양
    - 수집 방법
    - AI Hub 데이터 구축 기획서 참고
- 데이터 구축 기획서의 구성
  - 수업 생략

### 데이터 수집 1_직접 수집
- 수집 방법
  - 직접
    - 전문성 떨어져서 사이드 프로젝트 등에서 수행
  - 크롤링
  - 오픈소스
  - 크라우드 소싱
- 증강
  - 혼합 Belding
    - 유사 데이터를 합친다, 데이터 머징, 조인
  - 증강 Augmentation
    - back translation
    - image retation
  - 합성 Synthetic
    - 모델을 통해 생성

### 데이터 수집 2_크롤링 (이론)
- api 제공 여부 확인
- python library
  - requests
  - beautiful-Soup
  - scrapy
  - seleinum
  - pyquery

### 데이터 수집 2_크롤링 (실습)
- seleinum 을 이용한 이미지 크롤링
  - 구글이미지의 옵션을 설정하여 크롤링하는 예제
  - 크리에이티브 커먼즈 라이센스
- 실습은 생략

### 데이터 수집 3_오픈 소스
- 데이터 자체를 참고
- 데이터 수집 방법 참고
- 오픈 소스 데이터 플랫폼
  - 국내 
    - 공공데이터 포털
    - 서울 열린데이터광장
    - AI 허브
  - 해외
    - Kaggle
    - Huggingface
    - PapersWithCode
    - UCI ML Repo
    - 국가별 공공데이터 포털이 존재
      - data.gov.fr
      - data.gov.sg

### 데이터 수집 4_크라우드 소싱
- 업체선정해서 외주맡기는 형태

### 데이터 수집시 주의 사항 1_라이선스
- 코드
  - OSL
    - BSD
    - Apache License
    - MIT
- 데이터
  - CCL Creative Commons License
    - 저작권
    - 이용허락조건
      - 저작권자 표시
      - 비영리
      - 변경금지
      - 동일조건변경허락

### 데이터 수집시 주의 사항 2_개인정보보호
- 개인정보가 아닌경우
  - 사망한 정보
  - 집단의 통계값
- 개인정보인 경우
  - 개인에 대한 특정이 가능한경우

### 데이터 수집시 주의 사항 3_윤리

### 데이터 전처리
- 중복데이터는
  - 제거
  - 표시
- 결측데이터
  - 평균,중앙,최빈값을 사용하여 대체
  - 모델을 사용해서 예측
  - 제거
- 이상치 처리
  - 관측치에서 멀리 덜어진 값
  - 제거, 대체, 표시
- 도메인별 전처리
  - 이미지
    - 포맷 통일
    - 중복, 잘못찰영된 것 제거
  - 텍스트
    - "ㅋㅋㅋ", "ㅎㅎㅎ", 이모지등
      - 제거 또는 규측으로 처리
    - `utf-8` 인코딩으로 변호나
    - 어휘나 종류의 수가 미달인경우 제거등의 처리
- 데이터 스키마 설계
  - 원본데이터, 타스크 수행 결과, 메타데이터 등을 저장

### 라벨링 가이드라인 작성 방법
- 라벨링은 툴을 사용하기도 한다
  - 문서 같은 경우 Label Studio 등

### 데이터 라벨링 규칙 설정 1_CV
- 방식
  - 바운딩 박스
    - 직사각형, 타이트하게, 노이즈없이
  - 큐보인드
    - 3d 박스
  - 폴리곤
    - 다각형 형태의 바운딩 박스, 폐곡선
  - 폴리라인
    - 선형 객체의 경계나 위치등을 연속선으로 지정, 폐곡선이 아닌경우
  - 의미 분할
    - 픽셀 라벨링
  - 키 포인트
    - 객체의 주요 지점을 라벨링, 관절
- 모호성 제거
  - 띄어쓰기는 분리하여, 기호 분리등
  - `é` 와 같은 기호들을 라벨러가 통일성있게 기록할 수 있도록
  - 통일성이 가장 중요

### 데이터 라벨링 규칙 설정 2_NLP
- 기법
  - 분류 및 태깅
    - 다큐먼트(문단 등)의 분류
    - 텍스트의 부분을 태깅, NER
  - 전사
    - 음성 to 텍스트
    - 종류
      - 일반전사
        - 발음 전사
      - 이중 전사
        - 발음 전사와 철자(표준어) 전사 병기
      - 화자 전사
        - 화자 구분
- 사례
  - 방송 컨텐츠 전사(tool: zeroth)
- 번역 및 요약의 특징
  - 의미를 보존한체 텍스트를 변환하는 작업
  - 특별한 툴 없이 작업 가능
- 분류 또는 태깅시 고려할 사항
  - 태깅 대상 구분
    - 동일 군이 연속으로 등장하는 경우 태그를 합침
      - `Lorem` `Ipsum`
- 전사 규칙
  - 맞춤법 교육을 먼저받는다
  - 축약형, 방언은 이중 전사로 표기한다
  - 비식별화, 둘 이상인 경우 그 둘을 구분할 수 있어야한다
  - 주소는 동까지 표현
  - 이런것들은 규칙이 중요
- 요약
  - 몇 문장으로 요약해야하는지
  - 수정 불가능한 단어와, 수정 가능한 범주
  - 필수로 포함되어야하는 내용
    - 육하 원칙
  - 핵심 키워드가 2개이상 포함되어야한다

### 라벨링 툴 소개
- pass

### 데이터 클렌징 방법
- 라벨링 에러
  - 휴먼 에러
  - 라벨링 규칙 에러
    - 사람마다 다르게 해석 가능
    - 라벨링 중간에 규칙을 바꾸는 경우
- 데이터 클렌징
  - 규칙으로 정상화 할 수 있으면 좋지마 그렇지 않은 경우가 대다수
- 에러 확인
  - 샘플 직접 검수
    - 에러발견 -> 전체에서 해당 에러 필터링
  - 모델 결과 분석
  - IAA Inter Annotator Agreement
  - Confident Learning
    - 컨피던트가 낮은 데이터는 제외하고 사용
      - 제외한 데이터는 추가 검수
- 데이터 클렌징
  - 직접 클렌징
    - 라벨링 에러가 규칙적인지 않은 경우, 직접 클렌징
    - 비용, 휴먼에러 이슈
  - 규칙 기반 코드로 클렌징
- 라벨링 규칙 수정
  - 규칙 업데이트

### 데이터 평가 방법 - IAA
- 정의
  - IAA Inter Annotator Agreement
    - 작업자들간의 일치 정도
- 평가 방법들이 존재
  - 작업자들의 일치도록 평가하기 때문에 다수가 잘못된 선택을 하는 경우 보완

### IAA를 활용한 데이터 클렌징 방법
- pass

### 데이터 스플릿
- 과적합 방지
- 데이터 샘플링
  - 이점
    - 정확성, 편향 제거, 위험 감소
  - 방식
    - 확률적 샘프링
    - 비확률적 샘플링
  - 한계
    - 편향이 생길 수도 있다.

### 합성 데이터 - CV (이론)
- GANs
- VAE
- Diffusion

### 합성 데이터 - CV (실습)

### 합성 데이터 - NLP (이론)
- RNN
- BART
- LLM

### 합성 데이터 - NLP (실습)

### 액티브 러닝 (이론)
- 모델로 라벨링 -> 사람이 다시 라벨링 -> 라벨링 경계 업데이트

### 액티브 러닝 (실습)
- 생략

### 데이터 릴리즈
- 고려사항
  - 품질
  - 라이센스 권리표시
  - 개인정보 보호 문제
  - 윤리적 고려사항
- 릴리즈 플랫폼
  - https://paperswithcode.com/datasets
  - https://huggingface.co/datasets
  - https://kaggle.com/datasets

### CV 데이터 제작 실습
- 세그멘테이션 관련

### NLP 데이터 제작 실습
- 데이터 NER관련해서 해당 수동 태깅(툴 사용)하고 돌려보라는 뜻
- 생략

## link
- [[wn.private:upstage-ai-lab-day-102]]
- [[upstage-ai-lab]]
- [[upstage-ai-lab-day-98]]
